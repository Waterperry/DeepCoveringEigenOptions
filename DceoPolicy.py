import tensorflow as tf
from keras.optimizers import Adam
from keras.saving.legacy.save import load_model
from tensorflow.python.framework.errors_impl import InvalidArgumentError
from tensorflow.python.ops.gen_dataset_ops import MapDataset
from tf_agents.agents.dqn.dqn_agent import DdqnAgent, DqnAgent
from tf_agents.networks.q_network import QNetwork
from tf_agents.policies.py_policy import PyPolicy
import numpy as np
from tf_agents.replay_buffers import TFUniformReplayBuffer
from tf_agents.trajectories import time_step as ts, trajectory
from tf_agents.typing import types
from tf_agents.specs import ArraySpec
from tf_agents.trajectories import policy_step
from typing import Optional

import FourRooms
from GenericOptionAgent import PolicyOverNonGlobalOptions as Pongo, \
    DdqnAgentPseudo as DdqnOptionAgent, DqnAgentPseudo as DqnOptionAgent
from tf_agents.utils.common import element_wise_squared_loss
import HistoryHashTable
from PolicyModifiers import EGreedyPolicyWrapper
from tensorflow import keras
import math
from matplotlib import pyplot as plt


pseudo_reward_multiplier = 1  # multiply pseudo-rewards by this value.
drop_remainder = True       # whether to discard the remainder when batching datasets for training.
# For the below hyperparameters, YOU STILL NEED TO SET THEM WHEN CALLING THE CONSTRUCTOR.
#   These are just used in the helper functions, where we don't have direct reference to the policy.
num_options = 4     # number of options to use in the DCEO exploration policy.
dceo_batch_size = 128      # main and option policy batch size.


def generalized_laplacian_objective(y_true: tf.Tensor, y_pred: tf.Tensor):
    d = num_options

    # create a stack of state transitions
    f_s0 = y_pred[:-1]
    f_s1 = y_pred[1:]

    # first term of Generalized Laplacian Objective from (Wu et al., 2019)
    f_k_diff = f_s0 - f_s1
    term1_inner = tf.square(f_k_diff)
    term1 = tf.reduce_sum(term1_inner, axis=1)
    term1 = tf.concat((term1, tf.zeros(1)), axis=0)
    term1 *= 0.5

    l = 0
    term2 = tf.zeros(dceo_batch_size)
    while l < d:
        j = 0
        while j <= l:
            k = 0
            while k <= l:
                del_jk = 1. if j == k else 0.
                h_jk = (f_s0[:, j] * f_s0[:, k] - del_jk) * (f_s1[:, j] * f_s1[:, k] - del_jk)
                h_jk *= y_true[0] * y_true[1]
                term2 += tf.concat((h_jk, tf.zeros(1)), 0)
                k += 1
            j += 1
        l += 1

    return term1 + term2


# Some utility functions
def visualize_eigenfunction_using_nn(nn, print_goals=False, env='fourrooms'):
    if env == 'fourrooms':
        four_rooms = FourRooms.FourRooms()
        fr_shape = four_rooms.get_shape()
    elif env == 'ninerooms':
        four_rooms = FourRooms.NineRooms()
        fr_shape = four_rooms.get_shape()
    else:
        raise InvalidArgumentError(f"Environment {env} not implemented for visualization.")
    len_env = fr_shape[0] * fr_shape[1]

    # visualize eigenfunction
    eigs = np.zeros((num_options, len_env))
    for i in range(len_env):
        if i in four_rooms.get_walls():
            eigs[:, i] = -9e9
        else:
            # generate a valid example move to extract the eigenfunction value at each tile
            x = tf.reshape(tf.one_hot(i, len_env), (1, -1))
            x_dash = 0
            for move in four_rooms.get_adjacent_cells(i):
                if move != i:
                    x_dash = tf.reshape(tf.one_hot(move, len_env), (1, -1))
                    break
            unb_input = tf.stack((x, x_dash), axis=1)
            batched_input = tf.tile(unb_input, (dceo_batch_size, 1, 1))  # batch the input
            eig = nn.predict(batched_input, batch_size=dceo_batch_size, verbose=0)
            if np.any(np.isnan(eig)):
                print("NaN value generated by EF model. Consider training more.")
                return

            # eig[0] is our prediction, eig[0][0] is our prediction of f0(~)
            for e_idx in range(num_options):
                eigs[e_idx, i] = eig[0, e_idx].squeeze()

    valid_tiles = list(set(range(len_env)).difference(four_rooms.get_walls()))
    for e_idx in range(num_options):
        eigs[e_idx, four_rooms.get_walls()] = np.mean(eigs[e_idx, valid_tiles])
        option = np.where(eigs[e_idx] == np.min(eigs[e_idx]))[0], np.where(eigs[e_idx] == np.max(eigs[e_idx]))[0]
        if print_goals:
            tf.print(f"option_{e_idx}: ", option)

    if num_options < 4:
        n_rows = 1
        n_cols = num_options
        _ = plt.subplots(n_rows, n_cols)
    else:
        n_rows = int(math.sqrt(num_options))
        n_cols = int(math.sqrt(num_options))
        _ = plt.subplots(n_rows, n_cols)

    # fig.tight_layout()
    for i in range(num_options):
        eig_vis = eigs[i].reshape(fr_shape)
        plt.subplot(n_rows, n_cols, i + 1)
        _ = plt.contourf(eig_vis)

        if num_options <= 9:
            plt.title(f"EigFn {i}")
        plt.axis('off')

    plt.show()


# concatenates two trajectories and returns a single, longer trajectory.
def concatenate_trajectories(traj0: trajectory.Trajectory, traj1: trajectory.Trajectory):
    new_obs = tf.concat((traj0.observation, traj1.observation), 0)
    new_rwd = tf.concat((traj0.reward, traj1.reward), 0)
    new_st = tf.concat((traj0.step_type, traj1.step_type), 0)
    new_nst = tf.concat((traj0.next_step_type, traj1.next_step_type), 0)
    new_act = tf.concat((traj0.action, traj1.action), 0)
    if len(traj0.policy_info) != 0:
        raise NotImplementedError("Don't know what to do with non-zero policy...")
    else:
        new_pi = tf.concat((tf.zeros(1, tf.int32), tf.zeros(1, tf.int32)), 0)
    new_disc = tf.concat((traj0.discount, traj1.discount), 0)
    new_traj = trajectory.Trajectory(new_st, new_obs, new_act, new_pi, new_nst, new_rwd, new_disc)
    return new_traj


# substitute the actual reward in a trajectory with a pseudo-reward, calculated using the passed `network'.
#   zero_velocities is an additional parameter which would zero non-positional information before passing it to the
#   `network'. See the paper for more info.
def substitute_pseudo_reward(traj: trajectory.Trajectory, network: keras.Model, zero_velocities=False) \
        -> trajectory.Trajectory:
    if zero_velocities:
        new_obs = tf.concat((traj.observation[:, :, :2],
                             tf.zeros((traj.observation.shape[0], 2, 3), dtype=traj.observation.dtype)), axis=2)
        traj = trajectory.Trajectory(traj.step_type, new_obs, traj.action, traj.policy_info,
                                     traj.next_step_type, traj.reward, traj.discount)
        rwd = nn_pseudo_reward(traj, network)
    else:
        rwd = nn_pseudo_reward(traj, network)
    return trajectory.Trajectory(traj.step_type, traj.observation, traj.action,
                                 traj.policy_info, traj.next_step_type, rwd, traj.discount)


# Calculate the pseudo-reward using the given `network'.
def nn_pseudo_reward(traj: trajectory.Trajectory, network: keras.Model):
    observations = traj.observation  # (dceo_bs, 2)
    fs = network(observations)  # (dceo_bs, num_options)

    col0 = fs[:-1]  # f_i(s)
    col1 = fs[1:]  # f_i(s')
    reward_col0 = col1 - col0  # f_i(s') - f_i(s)

    # append zero b/c we don't know f(s') for last state.
    reward_col0 = tf.concat((reward_col0, tf.zeros((1, num_options))), 0)

    # make rewards a 128x2x[inner_dims] vector because we need a reward for each state...
    # this makes the assumption that we are going to follow policy at next step?
    reward_col1 = tf.concat((reward_col0[1:], tf.zeros((1, num_options))), 0)
    reward = tf.stack((reward_col0, reward_col1), axis=1)
    reward = tf.squeeze(reward)

    return reward * pseudo_reward_multiplier


class DceoPolicy(PyPolicy):
    def __init__(self,
                 # the following 5 variables are as standard in the TF-Agents framework
                 time_step_spec: ts.TimeStep, action_spec: types.NestedArraySpec, observation_spec: types.ArraySpec,
                 policy_state_spec=(ArraySpec((), np.int32), ArraySpec((), np.int32)),
                 replay_buffer_max_size=10000,  # maximum size of the agent's replay buffer
                 invalid_states=(),  # specify invalid states to prettify eigenfunction rendering (not required)
                 dceo_batch_size=128,  # batch size for option and main policies (MUST MATCH GLOBAL VARIABLE)
                 dceo_neuron_count=128,     # neuron count for the fully-connected layers of the EF Model
                 ef_nn_early_stopping_min_delta=1e-25,  # Min. Delta for Keras EarlyStopping Callback
                 main_n_steps=2,    # N-Steps for the main policy
                 adjust_sample_weights=False,   # Do importance sampling when training option policies off-policy
                 ef_nn_num_epochs=1000,  # number of epochs to train the eigenfunction network on
                 rng_seed=None,  # seed rng (useful for debugging)
                 dceo_loss_fn=generalized_laplacian_objective,  # can pass a custom loss function (like DCO) if wanted
                 num_options=4, # number of options to train
                 main_policy_train_steps=5,  # number of train steps executed on main when calling .train_policies()
                 option_train_steps=5,  # as above, for option policies
                 option_network_fc_layer_params=(75, 40),   # fully-connected layer params for option Q Networks
                 main_network_fc_layer_params=(75, 40),     # fully-connected layer params for main Q Network
                 option_target_update_tau=0.5,  # Tau for option target network updates
                 main_target_update_tau=0.5,    # Tau for main target network updates
                 option_target_update_period=25,
                 main_target_update_period=25,
                 mu=0.3,  # Probability of executing an exploratory option, given we are taking an explore step
                 D=10.0,  # The (1-in-X) chance of terminating option execution per time-step (D=10 -> 1/10 chance)
                 exploit_epsilon=0.1,   # The epsilon for the agent when in the exploit step (Phase 2)
                 verbose=False,
                 show_nn_train_output=False,    # Show Keras training output
                 restore_best_weights=False,    # Restore weights (in EarlyStopping Callback) - not necessary when
                                                #   using the Generalized Laplacian Obj. as it can't over-fit
                 debug=False,
                 show_actions=False,    # Verbosely print sampled actions/options (useful for debugging)
                 hash_function=lambda x: x,  # Hash function to transform states before putting them in the hashtable
                 zero_velocities_for_eigenfunction=False,  # Zero non-positional fields before giving to EF network
                 only_first_option=False,       # Only train the first option (useful for debugging)
                 use_ddqn_for_main_learner=False,   # Fairly self-explanatory...
                 use_ddqn_for_options=False
                 ):
        super().__init__(time_step_spec, action_spec, policy_state_spec)
        self._ht = HistoryHashTable.TranslatedHashTable(hash_function)
        self._main_n_steps = main_n_steps
        self._rng_seed = rng_seed
        self._mu = mu
        self._sa = show_actions
        self._D = D
        self._exploit_epsilon = exploit_epsilon
        self._rng = np.random.RandomState(seed=rng_seed)
        self._V = verbose or debug
        self._n_opt = num_options
        self._main_policy_train_steps = main_policy_train_steps
        self._dceo_batch_size = dceo_batch_size
        self.option_train_steps = option_train_steps
        self._dceo_neuron_count = dceo_neuron_count
        self._dceo_loss = dceo_loss_fn
        self.adjust_sample_weights = adjust_sample_weights
        self.invalid_states = invalid_states
        self.replay_buffer_max_length = replay_buffer_max_size
        self.restore_best_weights = restore_best_weights
        self._option_target_update_tau = option_target_update_tau
        self._option_target_update_period = option_target_update_period
        self._main_target_update_tau = main_target_update_tau
        self._main_target_update_period = main_target_update_period
        self._zero_v_before_nn = zero_velocities_for_eigenfunction
        self._explore_policy = Pongo(time_step_spec, action_spec, rng_seed=rng_seed,
                                     option_train_steps=option_train_steps, option_prob=self._mu,
                                     option_termination_prob=1. / self._D,
                                     adjust_sample_weights=adjust_sample_weights,
                                     verbose=self._V, show_actions=show_actions
                                     )
        self._exploit_policy_q_network = QNetwork(observation_spec,
                                                  action_spec,
                                                  fc_layer_params=main_network_fc_layer_params)
        if not use_ddqn_for_main_learner:
            self._exploit_policy = DqnAgent(time_step_spec, action_spec, self._exploit_policy_q_network,
                                            Adam(), epsilon_greedy=0.0, target_update_tau=self._main_target_update_tau,
                                            target_update_period=self._main_target_update_period,
                                            n_step_update=main_n_steps if main_n_steps != 2 else 1,
                                            td_errors_loss_fn=element_wise_squared_loss)
        else:
            self._exploit_policy = DdqnAgent(time_step_spec, action_spec, self._exploit_policy_q_network, Adam(),
                                             epsilon_greedy=0.0,
                                             n_step_update=main_n_steps if main_n_steps != 2 else 1,
                                             target_update_tau=main_target_update_tau,
                                             target_update_period=main_target_update_period,
                                             td_errors_loss_fn=element_wise_squared_loss)
        self._exploit_policy_wrapper = EGreedyPolicyWrapper(
            time_step_spec, action_spec,
            self._explore_policy, self._exploit_policy.policy,
            policy_state_spec=(ArraySpec((), np.int32), ArraySpec((), np.int32)),
            epsilon=self._exploit_epsilon,
            rng_seed=rng_seed,
            verbose=self._V,
            show_actions=show_actions)

        # potential issues if observation_spec.shape is not one integer, as it won't be in LunarLander
        obs_shape_spec = observation_spec.shape
        if len(obs_shape_spec) != 1:
            raise ValueError("Careful...")

        self._nn = tf.keras.models.Sequential(
            [
                tf.keras.layers.Input(shape=(2, obs_shape_spec[0])),
                tf.keras.layers.Dense(self._dceo_neuron_count, 'relu'),
                # tf.keras.layers.BatchNormalization(),
                # tf.keras.layers.Dropout(0.1),
                tf.keras.layers.Dense(self._dceo_neuron_count, 'relu'),
                # tf.keras.layers.BatchNormalization(),
                # tf.keras.layers.Dropout(0.1),
                tf.keras.layers.Dense(self._dceo_neuron_count, 'relu'),
                # tf.keras.layers.BatchNormalization(),
                # tf.keras.layers.Dropout(0.1),
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(self._n_opt)
            ]
        )
        self._nn.compile(optimizer=tf.keras.optimizers.legacy.Adam(), loss=self._dceo_loss, run_eagerly=debug)
        self.show_nn_train_output = show_nn_train_output
        self.ef_nn_early_stopping_min_delta = ef_nn_early_stopping_min_delta
        self.ef_nn_num_epochs = ef_nn_num_epochs
        self.exploration_phase = True
        self._debugging_one_option = only_first_option
        # add options to the exploit-policy's options array
        for i in range(self._n_opt):
            # create a PSR-DQN agent
            qn = QNetwork(observation_spec, action_spec, fc_layer_params=option_network_fc_layer_params)
            # set the agent's pseudo-reward function to a function which extracts the nth value from network's output
            if use_ddqn_for_options:
                dqn_agent = DdqnOptionAgent(time_step_spec, action_spec, qn, Adam(), lambda x: x,
                                            target_update_tau=self._option_target_update_tau,
                                            target_update_period=self._option_target_update_period,
                                            training_batch_size=self._dceo_batch_size)
            else:
                dqn_agent = DqnOptionAgent(time_step_spec, action_spec, qn, Adam(), lambda x: x,
                                           target_update_tau=self._option_target_update_tau,
                                           target_update_period=self._option_target_update_period,
                                           training_batch_size=self._dceo_batch_size)

            # make them globally available
            self._explore_policy.add_option(lambda _: 0.0 if only_first_option and i != 0 else 1.0, dqn_agent)

        # generic replay buffer, and pseudo-reward replay buffer
        self._option_ds = None
        self._option_ds_iter = None

        self._rb = TFUniformReplayBuffer(self._exploit_policy.collect_data_spec, 1, max_length=replay_buffer_max_size)
        self._rb_n_step_iter = iter(self._rb.as_dataset(self._dceo_batch_size, num_steps=main_n_steps+1))
        self._rb_iter = None

    def _action(self, time_step: ts.TimeStep, policy_state: types.NestedArray,
                seed: Optional[types.Seed] = None) -> policy_step.PolicyStep:
        if self._V:
            tf.print("Generating action.")

        if self.exploration_phase:
            if self._V:
                tf.print("\tExploratory action (exploration phase).")

            return self._explore_policy.action(time_step, policy_state, seed)

        else:
            return self._exploit_policy_wrapper.action(time_step, policy_state, seed)

    def train_ef_representation(self, iteration, visualize_ef=False, env='fourrooms'):
        if self._V:
            tf.print(f"Training internal representation.")
            tf.print("\tTransforming data.")

        buf = self._rb
        transitions: MapDataset = buf.as_dataset(num_steps=1, single_deterministic_pass=True) \
            .map(lambda x, _: x.observation)
        es_s = transitions.take(buf.num_frames() - 1)
        es_primes = transitions.skip(1).take(buf.num_frames() - 1)

        # zip the states together, so we can parallelize the rho-distribution calculations
        data = tf.data.Dataset.zip((es_s, es_primes))
        labels = data.map(lambda x, y: (self._ht.rho_distribution_of(x), self._ht.rho_distribution_of(y)))

        data = data.map(lambda s0, s1: tf.squeeze(tf.stack((s0, s1))))
        # higher batch size here might mean we discard less data
        train_dataset = tf.data.Dataset.zip((data, labels)).batch(batch_size=dceo_batch_size,
                                                                  drop_remainder=drop_remainder)
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='loss', min_delta=self.ef_nn_early_stopping_min_delta,
            patience=10, restore_best_weights=self.restore_best_weights)
        if self._V:
            tf.print("\tTraining model.")
        self._nn.fit(train_dataset, epochs=self.ef_nn_num_epochs,
                     callbacks=[early_stopping], verbose=1 if self._V or self.show_nn_train_output else 0)

        if visualize_ef:
            tf.print("Iteration", iteration, "visualize eigenfunctions.")
            visualize_eigenfunction_using_nn(self._nn, env=env)
        if self._V:
            tf.print("\tFinished training internal representation.")

    def train_policies(self):
        if self._rb_n_step_iter != 2:
            raise NotImplementedError("n-step training not implemented in train_policies(). Please use "
                                      "eagerly_train_policies() instead.")
        if self._V:
            tf.print("Training option policies.")

        if self._option_ds_iter is None:
            self.update_pseudo_reward_buffer()

        for train_step in range(self.option_train_steps):
            if self._V:
                tf.print("#", end="")
                if train_step % 5 == 0:
                    tf.print(" ", end="")
            try:
                experience_batch = next(self._option_ds_iter)
                self._explore_policy.train_on_experience_batch(
                    experience_batch, train_all_policies=not self._debugging_one_option)
            except StopIteration:
                if self._V:
                    tf.print("Aborting option training early (no samples left).")
                self._option_ds_iter = None
            except InvalidArgumentError:
                # tried to train without experience in the buffer.
                self._option_ds_iter = None
                return
            except TypeError:
                self._option_ds_iter = None

        # check we haven't exhausted all training samples
        if self._debugging_one_option:
            return
        if self._rb_iter is None:
            batched_dataset = self.get_rb_as_dataset(batched=True)
            ds_iter = iter(batched_dataset)
            self._rb_iter = ds_iter
        if self._V:
            tf.print("Training main learner.")
        for train_step in range(self._main_policy_train_steps):
            try:
                experience_batch = next(self._rb_iter)
                if self.adjust_sample_weights:
                    action_distribution = tf.nn.softmax(
                        self._exploit_policy_q_network.call(experience_batch.observation[:, 0])[0])
                    taken_action = experience_batch.action[:, 0]
                    indices = tf.stack((tf.range(self._dceo_batch_size), taken_action), axis=1)
                    weights = tf.gather_nd(action_distribution, indices)
                    self._exploit_policy.train(experience_batch, weights)
                else:
                    self._exploit_policy.train(experience_batch)
            except StopIteration:
                if self._V:
                    tf.print("No samples left in main training. Aborting training early.")
                self._rb_iter = None
                return
            except InvalidArgumentError:
                # tried to train without experience in the buffer.
                if self._V:
                    tf.print("No samples left in main training. Aborting training early.")
                self._rb_iter = None
                return

            # Legacy code to train on individual samples rather than a batch below
            if self._V:
                tf.print("Policy training Done.")
            # for unbatched_elem in range(self._dceo_batch_size):
            #
            #     # if we have received an incomplete batch due to end of buffer
            #     if experience_batch.observation.shape[0] <= unbatched_elem:
            #         break
            #
            #     replayed_ts = ts.TimeStep(experience_batch.step_type[:][unbatched_elem],
            #                               experience_batch.reward[:][unbatched_elem],
            #                               experience_batch.discount[:][unbatched_elem],
            #                               experience_batch.observation[:][unbatched_elem])
            #
            #     # call our q-net on the observation to get the suggested q-action
            #     # index 0, 0 because 1, state_0
            #
            #     if adjust_sample_weights:
            #         action_distribution = tf.nn.softmax(
            #             self._exploit_policy_q_network.call(replayed_ts.observation)[0][0])
            #
            #         # index train_step, 0 because experience_batch.action is a pair of actions...
            #         actual_action = experience_batch.action[unbatched_elem, 0]
            #
            #         # weights = tf.gather(action_distribution, actual_action)
            #         weights = action_distribution[actual_action]
            #         self._exploit_policy.train(experience_batch, weights)
            #     else:
            #         self._exploit_policy.train(experience_batch)

    def eagerly_train_policies(self):
        if self._rb_iter is None:
            self._rb_iter = iter(self.get_rb_as_dataset())

        try:
            for i in range(max(self.option_train_steps, self._main_policy_train_steps)):
                train_experience = None

                # if we are supposed to train option policy...
                if i < self.option_train_steps:
                    train_experience = next(self._rb_iter)
                    option_experience = substitute_pseudo_reward(train_experience, self._nn)
                    self._explore_policy.train_on_experience_batch(option_experience)

                # if we are supposed to train main policy...
                if i < self._main_policy_train_steps:
                    # if we're not using n-step, train using the same experience as the options
                    if self._main_n_steps == 2:
                        # if we're not training options, this will be None, so we need to get our own experience
                        if train_experience is None:
                            self._exploit_policy.train(next(self._rb_iter))
                            continue

                        # if train_experience isn't None, train with it.
                        self._exploit_policy.train(train_experience)

                    # otherwise, we're using n-step, so we need to get an experience from the n_step iterator.
                    train_exp = next(self._rb_n_step_iter)[0]
                    self._exploit_policy.train(train_exp)

        except StopIteration:
            self._rb_iter = None
            return
        except InvalidArgumentError:
            self._rb_iter = None
            return

    def set_exploration_phase(self):
        self.exploration_phase = True

    def clear_exploration_phase(self):
        self.exploration_phase = False

    def get_rb(self):
        return self._rb

    def get_rb_as_dataset(self, batched=True):
        buf = self._rb
        # if we really need to generate option dataset, do this. but it's very slow, so ideally we wouldn't have to.
        if self.option_train_steps != 0:
            buf_ds = buf.as_dataset(None, 1, single_deterministic_pass=True)
            # bufs = []
            # for i in range(self._main_n_steps):
            #     bufs.append(buf_ds.skip(i).take(buf.num_frames() - (self._main_n_steps - 1)).map(lambda x, _: x))

            buf0 = buf_ds.take(buf.num_frames() - 1).map(lambda x, _: x)
            buf1 = buf_ds.skip(1).take(buf.num_frames() - 1).map(lambda x, _: x)

            buf_ds = tf.data.Dataset.zip((buf0, buf1)).map(lambda x, y: concatenate_trajectories(x, y))

            if not batched:
                return buf_ds

            batched_dataset = buf_ds.batch(self._dceo_batch_size, drop_remainder=drop_remainder)
            return batched_dataset

        buf_ds = buf.as_dataset(None, 2)
        if not batched:
            return buf_ds
        return buf_ds.batch(self._dceo_batch_size, drop_remainder=drop_remainder)

    def save_model(self, model_name='./models/ef-model.h5'):
        self._nn.save(model_name)

    def load_model(self, env):
        if env == 'fourrooms':
            self._nn: keras.Model = load_model("./models/ef-model.h5", compile=False)
        elif env == 'ninerooms':
            self._nn: keras.Model = load_model("./models/ef-nine-rooms.h5", compile=False)
        elif env == 'fourrooms_cont':
            self._nn: keras.Model = load_model("./models/ef-fourrooms-cont.h5", compile=False)
        elif env == 'fourrooms_cont_noshaping':
            self._nn: keras.Model = load_model("./models/ef-cont-1mil-none.h5", compile=False)
        else:
            raise InvalidArgumentError(f"No model defined for env {env}.")
        self._nn.compile(Adam(), loss=self._dceo_loss)

    def set_model(self, model: keras.Model):
        self._nn = model
        self._nn.compile(Adam(), loss=self._dceo_loss)

    def get_model(self):
        return self._nn

    def get_batch_size(self):
        return self._dceo_batch_size

    def add_batch(self, traj):
        self._rb.add_batch(traj)
        self._ht(traj)

    def update_pseudo_reward_buffer(self):
        ds = self.get_rb_as_dataset(batched=True)

        self._option_ds = ds.map(lambda x: substitute_pseudo_reward(x, self._nn, self._zero_v_before_nn))
        self._option_ds_iter = iter(self._option_ds)

    def get_explore_policy(self):
        return self._explore_policy

    def get_exploit_policy(self):
        return self._exploit_policy_wrapper

    def override_replay_buffer(self, rb):
        self._rb = rb

    def set_exploit_epsilon(self, val):
        self._exploit_epsilon = np.float32(val)

    def get_exploit_epsilon(self):
        return self._exploit_epsilon
